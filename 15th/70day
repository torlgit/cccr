cluster - Performance	->	HPC(high performance computer)
	- HA

kvm 가상화, ovirt, openstack은 cluster
kvm 다 독립적이다 stand alone 

kvm단점, 관리의 어려움 개별적으로 작동하기 때문에 관리를 따로 해줘야한다.
ovirt시스템으로 통합해서 관리한다. 여러 대의 물리적인 시스템을 하나의 물리적 시스템인거 처럼 관리한다.

여러 개의 물리적인 시스템들이 통합된 네트워크를 가지게 된다.

kvm같은 경우 ovirt라는 시스템이 전체적으로 관리한다. = cluster
정보를 데이터 베이스 적용, 인터페이스를 웹에다 적용한다.

vmware, vspeier | vcenter, esxi(hyper visor)
gen server <- hyper visor)

openstack -> compute node 를 관리하는 controller node가 존재한다.

Performance :
성능을 극대화 시킨다. (병렬처리, 하나의 작업을 나눠주고 병렬로 처리해서 나중에 취합한다)
슈퍼컴퓨터 시뮬레이션, 병렬처리한다. 
빅데이터를 가공하기 위해서 hadop (cluster 개념) 데이터를 병렬처리해서 취합한다.

레드헷에서 HPC방식 OS가 있었고, 윈도우 HPC WINDOWS가 따로있었다. 지금은 사용하지 않는다.

HA :
고가용성, 아이티에서 제일 많이 사용하는 방식.
KVM hypervisor가 죽었을 때 다른 컴퓨터에서 띄울 수 있도록 하는 방식 (가용성, ha cluster)

docker = stand alone, 방식이라 고가용성으로 구축할 수 없다.
그래서 만들어진게 container cluster이다. 컨테이너를 클러스터로 관리를 하고 운영을 한다.

컨테이너를 클러스터링 하는 도구는 쿠버네티스만 있는게 아니다.
어쨋든 지금은 쿠버네티스가 시장에서 핫하지만, 아이티 기술은 언제 어떻게 될지 모른다.
쿠버네티스를 만든곳 -> google, 구글이 만들어서 인기가 있었다. 라는 측면도 있음

docker inc, docker swarm이 있다. (쿠버네티스보다 조금 늦게 나왔음)  | 도커만 클러스터 할 수 있음
아파치재단, foundation 메소스(mesos)

RKT(ROCKET), container runtime 표준화를 해서 표준화 하자. -> runc
runc container는 다른 runc에서도 작동한다.
즉 도커에서 사용하는 이미지를 다른 곳에서도 사용할 수 있다.

구글은 docker 이전부터 contianer를 썼으며 container를 borg로 관리를 했다.
이 개념으로 만든게 쿠버네티스이다.

K8S = Kubernetes, l10n (localisation)

aws, openstack|opensource

하나의 솔루션이 모든 기능을 제공하지 않는다.
openstack iaas
k8s paas

쿠버네티스가 제공해주는것
1. 컨테이너 가동을 지원해준다.
2. msa (마이크로서비스), container 개념에서 msa를 할 수 있다.
3. 이식성 있는 클라우드 플랫폼

이런 서비스를 오케스트레이션이라고 한다.
클러스터, 오케스트레이션 개념은 비슷하다. 오케스트레이션은 전체적으로 구성, 조율 = 클러스터가 하는 일

쿠버네티스가 paas = 개발환경을 제공, 인프라를 제공 | 가상컴퓨터를 제공
소프트웨어를 관리하고 운영할 수 있는 서비스

saas
소프트웨어까지 제공해주는 형태

쿠버네티스가 제공해주지 않는 기능
1. CI/CD (파이프라인) continuous 연속적인, 계속적인 끊임없이 하는 것
쿠버네티스, 도커 컨테이너 기반을 사용하는 경우, 개발자가 코드를 처음부터 개발, 기존에 있던 내용을 수정한 걸 github에 올리고 그거 컨테이너로 올라간다. 구성에 문제 없으면 릴리즈 (문제가 있으면 롤백한다) 쿠버네티스는 이러한 기능이 없다.

2. 어플리케이션 레벨의 서비스
saas가 아니기 때문에 container에 직접 작업해야한다.

3. 플랫폼 로깅, 모니터링, alert 관련된 솔루션은 없다.

시스템에 통합된 로그를 관리하고 분석할 수 있어야한다. 통합된 로그 서버를 제공하는 기능 자체가 없다.
쿠버네티스에서 제공하지 않는거지 ELK(elasticsearch logstack kibana) STACK, rates search(오픈소스)를 사용한다.

docker top 명령어를 사용해서 사용량을 봤었는데, container가 수 백대 수 만대 있으면 사용량을 보기 힘들다.
그래서 힙스터(모니터링 솔루션) -> 현재는 개발되지 않는다. 프로메테우스를 사용해서 cluster node 자체, container 사용량을 모니터링 할 수 있다.

모든 걸 컨테이너로 구현한다. k8s에도 컨테이너, 오픈스텍도 컨테이너 다 컨테이너에 올린다.
옛날에 apache package를 설치 했다면 지금은 모든 걸 컨테이너로 올린다.

paas에 모든 걸 kubernetes가 제공하지 않는다. 

마이크로서비스, 원래 있었던 개념
하나의 서비스를 여러가지 기능들이 있는데, 그걸 작게 작게 쪼게 놓은걸 마이크로서비스
마이크로, 아주 작은

서비스의 규모가 크지 않는 경우는 모놀리식 아키텍처를 하는게 낫다.

마이크로서비스는 프론트엔드에는 해당이 없다. -> 백엔드에만 해당이 된다.
tomcat - web container

war(웹 아카이브), 자바에서 사용
웹 어플리케이션을 만들고 각 각의 서비스를 하나의 war로 만든다.
service -> war -> node
서비스 변경 사항이 있으면, war를 다시 만들고 node를 해야한다.
was가 한 대라면 이러한 서비스도 할 수 없다.
모놀리식은 service 하나만 늘릴 수 없다.


service  ->  war  ->  node
로 하나로 묶어져있다. 만약 서비스 변경 사항이 있다면 War를 다시 만들고 node를 해야한다.
was 서버가 한 대 있다면 이러한 서비스도 할 수 없다. 
모놀리식은 service도 하나만 늘릴 수 없다. 기능 확장이 필요하면, 다시 service를 묶어 war를 만들어야한다.

마이크로서비스
서비스를 잘개 잘개 쪼게 놓는다.
msa는 컨테이너만 할 수 있는게 아니다. vm은 무거워서 안하는거다.
물리적이든 논리적이든 각 각의 서비스가 다른 vm, container 통신이 중요하다. 잘 이루어져야한다.
기존 시스템에서 msa로 바꾸는 형식은 어렵다. 통신하는 방식이 message queue를 사용해서 통신하거나 api를 이용해 통신한다.

기존 모놀리식에서는 이러한 통신 방식을 이용할 필요가 없다.
개발의 범위 늘어나고, 복잡해진다. 하지만 확장성이 좋다. / 통신이 복잡해진다.

마이크로서비스는 기존의 큰 서비스를 개별로 분리 시켰음. 컨테이너 처럼 나눠놨음
대신 각 서비스에 대한 통신이 되야하고 복잡하다.  

쿠버네티스 구성요소
openstack compute, controller 개념이 존재한다.

마스터, 미니언즈 두 가지로 나눠진다.
조그만한 미니언즈가 합쳐져서 마스터가 된다.

현재는 다른 이름을 사용한다.
master = controller plane : 조율, 관리를 한다.
minions[worker(node) = node] = openstack compute

master node
API Server, 분리가 되어있지 않지만 API server이다.
모든 통신을 API로 통신한다. / API server가 제어

etcd (etcd demon) :
유일한 저장소 모든 정보, 컨테이너의 이름 ~ 모든 클러스터에 관련된 정보 key valume storage | DB라고 말할 수 있다.
hash key valume, 저장소이다. 보통은 3개이상으로 구성한다.

kube-controller manager :
컨트롤러, 여러 개 존재한다. 컨트롤러가 마스터의 컨트롤러는 아니다.
컨테이너 생성, 관리를 한다. 컨트롤러의 종류는 다양하다. 수도 없이 많다. 컨테이너를 어떻게 관리하느냐에 다르다.

클라우드 컨트롤러 관리자(cloud controller manager) :
openstack에서는 매그넘 프로젝트가 있고, 쿠버네티스를 제공할 수 있다.
원래 쿠버네티스는 클라우드에 얹어서 쓰는 용도로 만들어지지 않았다. 그러다보니 클라우드 회사마다 소스코드를 건들이게 되었고 소스코드를 건들면 호환이 안되는 현상이 발생할 수 있으니 클라우드 컨트롤러 관리자를 만들었다.

kube-scheduler : 

node(worker) :
컨테이너 런타임이 존재하는게 핵심이다.

kubelet : 워커노드에 배치하는 에이전트, 마스터로 부터 명령을 받는 서비스

kube-proxy (프록시) : 네트워킹을 제공한다. 컨테이너에 네트워킹을 제공한다. 모든 워커 노드에 하나씩 배치 되어있어야 한다.

컨테이너 런타임 : docker를 사용함. docker말고도 여러가지가 존재한다. /  runc로 표준화됐다.
쿠버네티스에서 도커가 기본 런타임이다.

minikube : poc, test, 개발자가 사용하는 용도 (opensource) / 단일 node이다. 실제 서비스에선 사용 못한다.
microk8s : 리눅스에만 설치가 가능하고 단일 node이다.
kubeadm : 멀티 노드를 구성할 수 있다. 필요에 의해 올인원도 가능하다. / 멀티 노드 구성은 가능하지만 실제 서비스에선 쓰이지 않는다. 실제 서비스에서 사용하기엔 기능이 빈약하다.


minikube config set vm-driver kvm2 (defaults virtualbox)
curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-kvm2 \
  && sudo install docker-machine-driver-kvm2 /usr/local/bin/
minikube start
minikube stop
-p 옵션을 안쓰면 기본은 minikube

kubectl
cd .kube
cat config
원격 서버에 어떻게 접속할 수 있는지에 대한 내용이 담겨져있다.

kubectl cluster-info

아주 드물게 vm에 접속하고 싶다면 console id, password 모른다.
minokube ssh << key기반 인증으로 되어있다. / 마스터이자 워커이다.
sudo kbuerctl completion bash > /etc/bash_completion.d/kubectl1

명령어 정리
google search
kubernetes install
https://kubernetes.io/docs/tasks/tools/install-kubectl/

curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_1.3.1.deb  && sudo dpkg -i minikube_1.3.1.deb

ubuntu <<

minikube config set vm-driver kvm2

curl -LO https://storage.googleapis.com/minikube/releases/latest/docker-machine-driver-kvm2   && sudo install docker-machine-driver-kvm2 /usr/local/bin/

minikube start // option --memory=4000mb -p k8s
chmod +x ./kubectl
echo $PATH
sudo mv ./kubectl /usr/local/bin/kubectl
kubectl completion bash > /etc/bash_completion.d/kubectl1 | root <로 진행

sudo apt-get install     apt-transport-https     ca-certificates     curl     gnupg-agent     software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo add-apt-repository    "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable"
eval $(minikube docker-env)
export | grep DOCKER

sudo apt install nodejs


전 세계에서 제일 많이 쓰는 개발언어 = 자바
일반적으로 자바 스크립트라고 하면 하찮게 생각하는 경우가 있는데 자바 스크립트는 거의 모든 곳에 쓰인다.
nodejs 자바 스크립트 백엔드 용어, php 자바보다 빠르다.

npm = node package manager / 파이썬 패키지 매니저와 같음

vi index.js << node.js 어플리케이션 | 예제 참고 파일
vi Dockerfile | 작성
docker image build -t cccr:2.0 .

docker run -d -p 8080:8080 cccr:0.2
kubectl cluster-info
docker login
docker image tag cccr:0.2 tarl9123/cccr:0.2
docker image push tarl9123/cccr:0.2
책 46~49 참고

오브젝트 관리
명령형 오브젝트 구성 : 
모든 리소스는 yaml로 만든다.

선언형 오브젝트 구성 :
리소스 오브젝트가 여러 개 있을 수 있는데, 그 파일을 특정 디렉토리에 넣어놓고 실행.

쿠버네티스에서는 컨테이너라는 개념을 쓰지 않고 파드라고 한다.
쿠버네티스 파드, 파드란 어플리케이션을 돌릴 수 있는 최소 단위이며 파드안에 컨테이너가 존재한다. = contaier
삭제하면 새로운 파드가 생성이 된다. 하나의 파드에 하나의 컨테이너를 갖고 있는게 제일 이상적이지만 여러 개도 갖고 있을 수 있다.

get = 정보를 확인한다.

kubectl run cccr-app --image=tarl9123/cccr:0.2 --port=8080 --generator=run/v1
watch -n1 -d kubectl get pods,replicationcontroller
kubectl expose replicationcontroller cccr-app --type=LoadBalancer --name cccr-svc
kubectl get serivces
sudo minikube tunnel // 미니 쿠버의 기능
kubectl get service
service가 로드밸런스 기능을 갖고 있다. Port 앞에는 내부 뒤에는 host

미니쿠버는 멀티 노드가 안되는게 단점이다.
kubeadm 설치하는 도구 절차가 있다.

멀티노드를 구성하기 위해서 master 1대 worker 2대
vm 3대를 준비한다. cpu 2개 메모리 2기가 이상

kubeadm 자체는 아주 쉽다. 몇개 없다.
미리 구성해야하는 환경 구성이 있다.
before you begin에 나옴
centos7 or ubuntu 16.04 이상
모든 가상컴퓨터가 통신이 되야하고, hostname도 세팅이 필요. swap이 없어야한다. | lvm으로 구현되어 있어서 fs에서 제거를 하든 vm을 다시 설치해서 swap을 설치 하지 않는 구성으로. fstab에서 swap을 제거하면된다.
swap 이 있으면 느려지기 때문에 제거를 해야한다.
외부서비스 , 내부서비스 분리할 필요가 있지만, nat network setting, static ip로 구성 (난이도가 높다)

Check required ports
firewalld Control-plane node(s) < port가 열려있어야함
Worker node(s) < port가 열려있어야함

Installing runtime
master, worker | docker-ce 를 설치해야한다. 기본 docker는 버전이 낮다. << ce 설치
service 활성화 <<

Installing kubeadm, kubelet and kubectl
centos, debian 둘 중 선택.

쿠버네티스가 완전히 selinux 지원을 안해서 설치 시에는 permissive mode로 변경
그 후에는 가이드 대로 설치 (master, worker 모두 다)


cat <<EOF >  /etc/sysctl.d/k8s.conf 에
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
두 줄 추가

포트포워딩, sysctl 커널의 파라미터를 조정한다.
lsmod | grep br_netfilter (커널의 모듈, 기능 중 br_netfilter 기능이 있어야한다) / 아마 있을거다.

여기까지 준비 단계

kubeadm init --pod-network-cidr=x.x.x.x/x | pod network 선언
kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml (worker까지 다 끝내면)
calico << 

mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

sudo로 작업을 했다면 위와 같이 해야한다.

worker << 만 마스터에 연결 시키세요 라는 명령어 | 마스터 구성이 잘 되었으면 무조건된다.
kubeadm join <master-ip>:<master-port> --token <token> --discovery-token-ca-cert-hash sha256:<hash>
공식 문서를 보고 설치해보고 안되면 블로거를 참고, 공식적인 문서를 보고 -> error 해결 방법만 보자

관리형 쿠버네티스, = 마스터를 제어할 수 없다.
마스터를 제어를 하려면 설치형을 써야함.

로드밸런서 elb -> 관리형, haproxy를 쓰지만 모든 기능추가, 변경이 안된다.
라우터, 게이트웨이도 관리형이다. 아마존에서 제공해주는 기능만 사용이 가능하다.
